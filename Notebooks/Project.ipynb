{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c38e5d-829a-445a-bde2-748cf8e49caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backup of current work just in case of loss of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f48bea-0f43-4c84-918f-c7f9f1be59c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "from ast import literal_eval\n",
    "from pandas_profiling import ProfileReport\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.models import save_model, load_model, model_from_json\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, TextVectorization, Input, concatenate, StringLookup, BatchNormalization, Flatten, Lambda\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f93da8-f32f-4699-a57b-0233d7e6c6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data from: https://webrobots.io/kickstarter-datasets/ * Used in current model\n",
    "# Reads in all files in a directory(make sure that all files are csv)\n",
    "files = glob.glob(\"data/Kickstarter_2021-01-14T03_20_05_328Z/*\")\n",
    "df = pd.concat(map(pd.read_csv, files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ace44f-c3ea-4518-a074-11dd8fa1743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://www.kaggle.com/sripaadsrinivasan/kickstarter-campaigns-dataset\n",
    "df = pd.read_csv('data/kickstarter_data_full.csv', low_memory=False).drop(columns=['Unnamed: 0'])\n",
    "columns = ['blurb', 'category', 'country', 'currency', 'goal', 'name', 'state']\n",
    "df = df[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9177e5-5ec9-429e-9dc7-efe892992b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Profile report for data exploring\n",
    "profile = ProfileReport(df, minimal=True)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76ce3d-cc6e-4882-8f08-dab6544cc243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping unneeded colums\n",
    "columns = ['friends', 'is_backing', 'is_starred', 'permissions', 'is_starrable', 'urls', 'creator', 'photo', 'profile', 'source_url', 'backers_count', 'converted_pledged_amount', 'created_at', 'location', 'deadline', 'currency_symbol', 'disable_communication', 'id', 'launched_at', 'pledged', 'slug', 'spotlight', 'staff_pick', 'state_changed_at', 'currency_trailing_code', 'fx_rate', 'current_currency', 'usd_pledged', 'static_usd_rate', 'country_displayable_name','usd_type']\n",
    "df = df.drop(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a5214-6304-4143-a7b1-ca85dba0bebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Changing the stringified dictionary to a dictionary and only returning id 'name'\n",
    "# swifter allows faster .apply() method for pandas\n",
    "df['category'] = df['category'].swifter.apply(lambda x: literal_eval(x)['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9c035-9e7d-4c64-9060-0c66ee235b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save current data to csv so no time is wasted recleaning data\n",
    "df = df[['name', 'blurb', 'goal', 'category', 'country', 'currency', 'state']]\n",
    "df.to_csv('data/cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf3be0-5d83-4d16-80dc-4102d07a7d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned_dataset.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b070fe-da11-463d-875a-1d38d7425b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtering the dataset to make target column binary\n",
    "df_binary = df.loc[(df['state']=='successful') | (df['state']=='failed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabe771-c853-4b6c-8984-0e849262993e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used for making the vocab for a layer\n",
    "seq_len = 40\n",
    "cat_feat = df_binary.drop(columns=['blurb', 'name', 'state', 'goal'])\n",
    "# Chaning from string to integer values\n",
    "y = df_binary['state']\n",
    "y = y.replace('successful', 1)\n",
    "y = y.replace('failed', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd4d4b-e850-4348-9c0d-d6773041a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeing vocab\n",
    "vocab = set()\n",
    "for cols in cat_feat.columns:\n",
    "    for row in cat_feat[cols]:\n",
    "        vocab.add(row)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd5134-40a9-45c1-bd65-4462ad7a0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization and embedding for 'blurb' to save preprocessing time\n",
    "text_vect_obj = TextVectorization(max_tokens=20000, output_sequence_length=40, pad_to_max_tokens=True)\n",
    "# Adapt creates the vocab list quickly for the TextVectorization layer(cleaner than making a function to do it)\n",
    "text_vect_obj.adapt(df_binary['blurb'].astype(str))\n",
    "# Text vectorization and embedding for 'name' to save preprocessing time\n",
    "text_vect_obj2 = TextVectorization(max_tokens=20000, output_sequence_length=40, pad_to_max_tokens=True)\n",
    "text_vect_obj2.adapt(df_binary['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a8d68-f8f9-4d51-83ed-3227db95ee9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "nlp_blurb = Input(shape=(1,), dtype=tf.string)\n",
    "nlp_name = Input(shape=(1,), dtype=tf.string)\n",
    "nlp_cols = Input(shape=(3,), dtype=tf.string)\n",
    "meta_input = Input((1,), dtype=tf.float32)\n",
    "\n",
    "text_vect = text_vect_obj(nlp_blurb)\n",
    "embed = Embedding(20000, 50)(text_vect)\n",
    "nlp_blurb_out = LSTM(500)(embed)\n",
    "\n",
    "text_vect2 = text_vect_obj2(nlp_name)\n",
    "embed_2 = Embedding(20000, 50)(text_vect2)\n",
    "nlp_name_out = LSTM(500)(embed_2)\n",
    "\n",
    "# Encoding the columns the columns that can be multi hot encoded\n",
    "cat_encoding = StringLookup(output_mode='multi_hot', vocabulary=vocab)(nlp_cols)\n",
    "\n",
    "# Normalizing 'goal' to save preprocessing time\n",
    "norm = BatchNormalization()(meta_input)\n",
    "\n",
    "# Concatinates the outputs for the nlp models, multi hot model and normilization model\n",
    "num_cat = concatenate([nlp_blurb_out, nlp_name_out, cat_encoding, norm])\n",
    "\n",
    "# Standard Dense model with max_norm constraints and Dropout for weight regularization\n",
    "x = Dense(1000, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(num_cat)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(750, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(500, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(250, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Inputs need to have all of the inputs above in list format [input1, input2, ...]\n",
    "model = Model(inputs=[nlp_blurb, nlp_name, meta_input, nlp_cols], outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2cf7ed-fc8d-4da1-997e-9a9231d12988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data is going to be in list format, not in single dataframe as shown here [df[col1], df[col2], ...]\n",
    "# Input order matters\n",
    "# Using validation split and Earlystopping\n",
    "model.fit([df_binary['blurb'].astype('string'), df_binary['name'].astype('string'), df_binary['goal'], df_binary[['category', 'country', 'currency']].astype('string')], y, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76384c9-4424-4e12-9e48-76d0d531de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file size ~80MB\n",
    "save_model(model, 'model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9629101-f6e6-4ab8-aec7-7b669973585a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Just a visualization of the model from above\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1b5c8b-be89-4b6d-b2af-faebbb3213f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e95cab-8a02-4b2a-a1f2-d32a7aba430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vect_obj = TextVectorization(max_tokens=20000, output_sequence_length=40, pad_to_max_tokens=True)\n",
    "text_vect_obj.adapt(df_binary['blurb'])\n",
    "text_vect_obj2 = TextVectorization(max_tokens=20000, output_sequence_length=40, pad_to_max_tokens=True)\n",
    "text_vect_obj2.adapt(df_binary['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea0383-c474-4b85-a6cf-15348595b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(opt='adam', lr=0.0001, nodes=1000):\n",
    "    big_in = Input(5,)\n",
    "    nlp_name = Lambda(lambda x: tf.cast(tf.expand_dims(x[:,0],-1), dtype=tf.string))(big_in)\n",
    "    nlp_blurb = Lambda(lambda x: tf.cast(tf.expand_dims(x[:,1],-1), dtype=tf.string))(big_in)\n",
    "    meta_input = Lambda(lambda x: tf.cast(tf.expand_dims(x[:,2],-1), dtype=tf.float32))(big_in)\n",
    "    nlp_cols = Lambda(lambda x: tf.cast(x[:,3:], dtype=tf.string))(big_in)\n",
    "    \n",
    "    text_vect = text_vect_obj(nlp_blurb)\n",
    "    embed = Embedding(len(text_vect_obj.get_vocabulary()), 50)(text_vect)\n",
    "    nlp_blurb_out = LSTM(500)(embed)\n",
    "    text_vect2 = text_vect_obj2(nlp_name)\n",
    "    embed_2 = Embedding(len(text_vect_obj.get_vocabulary()), 50)(text_vect2)\n",
    "    nlp_name_out = LSTM(500)(embed_2)\n",
    "    cat_encoding = StringLookup(output_mode='multi_hot', vocabulary=vocab)(nlp_cols)\n",
    "    norm = BatchNormalization()(meta_input)\n",
    "    num_cat = concatenate([nlp_blurb_out, nlp_name_out, cat_encoding, norm])\n",
    "    \n",
    "    x = Dense(nodes, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(num_cat)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(nodes*0.75, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(nodes*0.5, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(nodes*0.25, activation='relu', kernel_constraint=max_norm(), bias_constraint=max_norm())(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=big_in, outputs=x)\n",
    "    \n",
    "    if opt == 'adam':\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif opt == 'rmsprop':\n",
    "        model.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=lr), metrics=['accuracy'])\n",
    "    elif opt == 'adagrad':\n",
    "        model.compile(loss='binary_entropy', optimizer=Adagrad(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788cb10-5ca0-468e-b55e-63b5b22a2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'opt': ['adam', 'rmsprop', 'adagrad'],\n",
    "    'lr': [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'nodes': [500, 1000, 2000, 4000]\n",
    "}\n",
    "wrapper = KerasClassifier(build_fn=build_model, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)])\n",
    "search = RandomizedSearchCV(wrapper, param_grid, cv=3, n_iter=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3f2d7-a463-4f5a-96fe-b6f3490716c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search.fit(df_binary[['name', 'blurb', 'goal', 'category', 'country' ,'currency']], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737eeba6-cea0-4222-ba52-a818ab8ed06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
